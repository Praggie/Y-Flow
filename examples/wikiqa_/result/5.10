Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=2, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3=None, with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 2619.48 (366.101 sec)
valid- map: '0.532917053392', mrr: '0.538913001154'
test- map: '0.558644258837', mrr: '0.564628106951

Step 873: loss = 2135.07 (383.237 sec)
valid- map: '0.577105922011', mrr: '0.582425271974'
test- map: '0.583672111084', mrr: '0.594270473766

Step 1310: loss = 1446.15 (379.316 sec)
valid- map: '0.650809147366', mrr: '0.65809403663'
test- map: '0.665977421574', mrr: '0.675703447753

Step 1747: loss = 985.62 (378.749 sec)
valid- map: '0.692855083629', mrr: '0.697724552784'
test- map: '0.684680026368', mrr: '0.703663843016

Step 2184: loss = 693.60 (378.372 sec)
valid- map: '0.673835611038', mrr: '0.679573832252'
test- map: '0.678185616979', mrr: '0.689542396333

Step 2621: loss = 442.33 (379.578 sec)
valid- map: '0.659716815372', mrr: '0.668116286271'
test- map: '0.670329590001', mrr: '0.681590041369

Step 3058: loss = 304.59 (379.594 sec)
valid- map: '0.656135332921', mrr: '0.657927346023'
test- map: '0.648031138236', mrr: '0.659643631403

Step 3495: loss = 229.54 (378.251 sec)
valid- map: '0.659975742218', mrr: '0.666592457366'
test- map: '0.652659111675', mrr: '0.661558636232

Step 3932: loss = 228.39 (376.852 sec)
valid- map: '0.661395130443', mrr: '0.670811573788'
test- map: '0.666306245225', mrr: '0.677068339934

Step 4369: loss = 177.94 (377.154 sec)
valid- map: '0.654216572669', mrr: '0.661945219683'
test- map: '0.67011633304', mrr: '0.681483747633

Step 4806: loss = 167.47 (378.526 sec)
valid- map: '0.668167289408', mrr: '0.682893920801'
test- map: '0.650378453851', mrr: '0.665403520805

Step 5243: loss = 138.74 (375.203 sec)
valid- map: '0.676935916817', mrr: '0.691637837471'
test- map: '0.653371333078', mrr: '0.665208217889

Step 5680: loss = 141.52 (378.178 sec)
valid- map: '0.656159504076', mrr: '0.659735425509'
test- map: '0.668194678802', mrr: '0.680802423994

Step 6117: loss = 139.02 (376.084 sec)
valid- map: '0.682991138051', mrr: '0.69080954319'
test- map: '0.664573795676', mrr: '0.674282747431

Step 6554: loss = 147.44 (377.849 sec)
valid- map: '0.677491694684', mrr: '0.68565736836'
test- map: '0.660070498394', mrr: '0.671038843415

train- map: '0.992206738598', mrr: '0.996029018709'
