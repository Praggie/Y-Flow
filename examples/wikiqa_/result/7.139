Namespace(MP_dim=50, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=100, attention_type='linear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, cnf=3, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=False, is_aggregation_siamese=True, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=13, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='mul', type2='mul', type3=None, unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=True, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 874.88 (41.345 sec)
valid- map: '0.649511650901', mrr: '0.649736308665'
test- map: '0.577375176985', mrr: '0.586586433866

Step 545: loss = 769.15 (45.687 sec)
valid- map: '0.659718595631', mrr: '0.665177569344'
test- map: '0.598968985698', mrr: '0.601143238335

Step 818: loss = 691.59 (45.858 sec)
valid- map: '0.71789049795', mrr: '0.72675608122'
test- map: '0.649694427955', mrr: '0.662877840038

Step 1091: loss = 608.70 (45.261 sec)
valid- map: '0.688331729998', mrr: '0.692205501729'
test- map: '0.636524606954', mrr: '0.642436493846

Step 1364: loss = 539.41 (45.121 sec)
valid- map: '0.700892857143', mrr: '0.704859536407'
test- map: '0.660929638231', mrr: '0.668613152003

Step 1637: loss = 482.84 (44.840 sec)
valid- map: '0.679851233423', mrr: '0.683995567924'
test- map: '0.650934026957', mrr: '0.658855650522

Step 1910: loss = 417.44 (45.612 sec)
valid- map: '0.713315626521', mrr: '0.721165525739'
test- map: '0.637324135702', mrr: '0.650422999497

Step 2183: loss = 367.46 (45.816 sec)
valid- map: '0.726796252689', mrr: '0.733033655057'
test- map: '0.653096069454', mrr: '0.661009121194

Step 2456: loss = 307.99 (45.472 sec)
valid- map: '0.714158449575', mrr: '0.723978303443'
test- map: '0.632543881206', mrr: '0.644788807443

Step 2729: loss = 272.80 (44.436 sec)
valid- map: '0.71951845553', mrr: '0.725130700428'
test- map: '0.657058351221', mrr: '0.668982088408

Step 3002: loss = 217.06 (44.372 sec)
valid- map: '0.728549154705', mrr: '0.738038320741'
test- map: '0.673983138339', mrr: '0.681780325299

Step 3275: loss = 183.42 (44.544 sec)
valid- map: '0.693431035803', mrr: '0.700380204359'
test- map: '0.659130810617', mrr: '0.66733617891

Step 3548: loss = 168.37 (45.188 sec)
valid- map: '0.69109722949', mrr: '0.695958340601'
test- map: '0.655090679307', mrr: '0.661529851036

train- map: '0.926272257489', mrr: '0.937590683467'
