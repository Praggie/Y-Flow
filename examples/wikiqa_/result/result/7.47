Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=5, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_mul', with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1433.58 (389.451 sec)
valid- map: '0.581437279652', mrr: '0.584797411583'
test- map: '0.586783698126', mrr: '0.599991480856

Step 873: loss = 1235.30 (394.839 sec)
valid- map: '0.649250741478', mrr: '0.663750857763'
test- map: '0.636560646979', mrr: '0.648363442982

Step 1310: loss = 944.04 (391.116 sec)
valid- map: '0.693051921623', mrr: '0.697605965463'
test- map: '0.670580890706', mrr: '0.68503949183

Step 1747: loss = 736.21 (394.299 sec)
valid- map: '0.689260805332', mrr: '0.690120593692'
test- map: '0.673412082022', mrr: '0.684042451

Step 2184: loss = 477.13 (393.024 sec)
valid- map: '0.690819564034', mrr: '0.698765520194'
test- map: '0.675405429871', mrr: '0.686866579922

Step 2621: loss = 283.83 (402.434 sec)
valid- map: '0.681657848325', mrr: '0.683122323003'
test- map: '0.708270317087', mrr: '0.720353937329

Step 3058: loss = 153.39 (408.360 sec)
valid- map: '0.706262399417', mrr: '0.710333009143'
test- map: '0.702406982', mrr: '0.715407751828

Step 3495: loss = 111.68 (408.056 sec)
valid- map: '0.693820377154', mrr: '0.697240634145'
test- map: '0.692413713771', mrr: '0.70056637632

Step 3932: loss = 87.89 (408.158 sec)
valid- map: '0.68500627459', mrr: '0.692815231506'
test- map: '0.693886061322', mrr: '0.704224104052

Step 4369: loss = 79.15 (404.268 sec)
valid- map: '0.69883895866', mrr: '0.699429472644'
test- map: '0.684010246559', mrr: '0.698972974899

Step 4806: loss = 76.89 (409.446 sec)
valid- map: '0.678427216522', mrr: '0.682341357937'
test- map: '0.667524718334', mrr: '0.681309642575

Step 5243: loss = 76.21 (405.474 sec)
valid- map: '0.705545622808', mrr: '0.711542095471'
test- map: '0.675191410335', mrr: '0.689738062578

Step 5680: loss = 69.08 (412.125 sec)
valid- map: '0.698429759109', mrr: '0.705080281271'
test- map: '0.693728258481', mrr: '0.704031273939

Step 6117: loss = 49.51 (418.581 sec)
valid- map: '0.700131592393', mrr: '0.71305203835'
test- map: '0.69348703241', mrr: '0.701896805755

Step 6554: loss = 50.37 (417.929 sec)
valid- map: '0.696388377936', mrr: '0.705281072543'
test- map: '0.696737241585', mrr: '0.710943940785

train- map: '0.997064492082', mrr: '0.999427262314'
