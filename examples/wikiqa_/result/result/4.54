Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=5, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_mul', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 2626.88 (403.733 sec)
valid- map: '0.514489885625', mrr: '0.51948799598'
test- map: '0.517203849046', mrr: '0.529293937952

Step 873: loss = 2675.64 (418.656 sec)
valid- map: '0.513125630983', mrr: '0.522271753819'
test- map: '0.478476950018', mrr: '0.486157684517

Step 1310: loss = 2601.31 (419.827 sec)
valid- map: '0.5432307221', mrr: '0.552301016884'
test- map: '0.509685226749', mrr: '0.518186694115

Step 1747: loss = 2465.64 (420.148 sec)
valid- map: '0.606544630761', mrr: '0.621458993295'
test- map: '0.586009772566', mrr: '0.595120364855

Step 2184: loss = 2352.63 (422.437 sec)
valid- map: '0.613026790447', mrr: '0.616340514927'
test- map: '0.578873489907', mrr: '0.5935072986

Step 2621: loss = 1973.16 (418.392 sec)
valid- map: '0.661770621224', mrr: '0.662906816443'
test- map: '0.644305266065', mrr: '0.660476235013

Step 3058: loss = 1365.26 (420.962 sec)
valid- map: '0.635496492044', mrr: '0.638119948834'
test- map: '0.627718411825', mrr: '0.639852094329

Step 3495: loss = 948.39 (418.486 sec)
valid- map: '0.666999193487', mrr: '0.674721541686'
test- map: '0.649027405686', mrr: '0.665370015216

Step 3932: loss = 696.93 (420.673 sec)
valid- map: '0.670269104793', mrr: '0.678451266547'
test- map: '0.665938017365', mrr: '0.681285373589

Step 4369: loss = 522.25 (420.381 sec)
valid- map: '0.650223409747', mrr: '0.65212694052'
test- map: '0.660495279091', mrr: '0.673857302118

Step 4806: loss = 349.58 (421.392 sec)
valid- map: '0.679951412059', mrr: '0.682680374014'
test- map: '0.676183553269', mrr: '0.693194617752

Step 5243: loss = 287.11 (419.794 sec)
valid- map: '0.701391553773', mrr: '0.707111363064'
test- map: '0.66017280001', mrr: '0.67533573909

Step 5680: loss = 242.98 (421.299 sec)
valid- map: '0.665370487097', mrr: '0.665657083217'
test- map: '0.656519878739', mrr: '0.674993054797

Step 6117: loss = 187.70 (419.784 sec)
valid- map: '0.654615387166', mrr: '0.660920986328'
test- map: '0.645567841639', mrr: '0.66107981897

Step 6554: loss = 152.33 (419.852 sec)
valid- map: '0.687553271591', mrr: '0.689934223972'
test- map: '0.653813077451', mrr: '0.67355027604

train- map: '0.99182710642', mrr: '0.998281786942'
