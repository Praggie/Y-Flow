Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_mul', with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1294.66 (416.662 sec)
valid- map: '0.646066818983', mrr: '0.64767774113'
test- map: '0.617116417713', mrr: '0.632817074329

Step 873: loss = 999.78 (432.934 sec)
valid- map: '0.691359114871', mrr: '0.695705297789'
test- map: '0.646864939691', mrr: '0.660141923087

Step 1310: loss = 727.67 (435.190 sec)
valid- map: '0.680879746356', mrr: '0.681580203306'
test- map: '0.680009954218', mrr: '0.69041974829

Step 1747: loss = 462.75 (442.124 sec)
valid- map: '0.69126316356', mrr: '0.692352333796'
test- map: '0.675087810684', mrr: '0.684545398589

Step 2184: loss = 278.33 (440.910 sec)
valid- map: '0.686765989742', mrr: '0.695138404365'
test- map: '0.697039081305', mrr: '0.707350965221

Step 2621: loss = 163.17 (439.767 sec)
valid- map: '0.672294962881', mrr: '0.676387618462'
test- map: '0.676547743373', mrr: '0.685487106629

Step 3058: loss = 95.27 (437.430 sec)
valid- map: '0.680264708018', mrr: '0.690075598186'
test- map: '0.67211204774', mrr: '0.68230252258

Step 3495: loss = 90.30 (437.380 sec)
valid- map: '0.7047473062', mrr: '0.709202143691'
test- map: '0.671222511094', mrr: '0.684057774644

Step 3932: loss = 73.48 (435.726 sec)
valid- map: '0.719559001404', mrr: '0.72469144612'
test- map: '0.679325871732', mrr: '0.690276041974

Step 4369: loss = 80.14 (433.602 sec)
valid- map: '0.692537599085', mrr: '0.700380109309'
test- map: '0.686212573615', mrr: '0.69581021973

Step 4806: loss = 59.96 (433.885 sec)
valid- map: '0.679803254506', mrr: '0.68393576221'
test- map: '0.691037287199', mrr: '0.696691637124

Step 5243: loss = 78.46 (433.299 sec)
valid- map: '0.717490779658', mrr: '0.72070789984'
test- map: '0.671053126368', mrr: '0.683614779139

Step 5680: loss = 65.15 (432.160 sec)
valid- map: '0.688386330316', mrr: '0.690155207382'
test- map: '0.66258203413', mrr: '0.677748994261

Step 6117: loss = 59.33 (432.988 sec)
valid- map: '0.70904704863', mrr: '0.711389117937'
test- map: '0.668628300288', mrr: '0.671736294113

Step 6554: loss = 48.00 (432.552 sec)
valid- map: '0.697930354478', mrr: '0.704639077853'
test- map: '0.684625969217', mrr: '0.693772499791

train- map: '0.997408064762', mrr: '1.0'
