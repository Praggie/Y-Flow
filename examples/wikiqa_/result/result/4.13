Namespace(MP_dim=50, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1811.55 (398.538 sec)
valid- map: '0.692747001676', mrr: '0.698951048951'
test- map: '0.699740185374', mrr: '0.713647787007

Step 873: loss = 1125.07 (416.223 sec)
valid- map: '0.696396879135', mrr: '0.702440306607'
test- map: '0.732825417051', mrr: '0.745406862228

Step 1310: loss = 600.25 (416.925 sec)
valid- map: '0.677812828706', mrr: '0.680778480778'
test- map: '0.70724735798', mrr: '0.72310395366

Step 1747: loss = 294.23 (417.514 sec)
valid- map: '0.676517331874', mrr: '0.686223520747'
test- map: '0.726967646128', mrr: '0.738592123469

Step 2184: loss = 187.11 (415.266 sec)
valid- map: '0.667922971658', mrr: '0.669848309694'
test- map: '0.722598383387', mrr: '0.734860823904

Step 2621: loss = 163.83 (415.618 sec)
valid- map: '0.708721283126', mrr: '0.711073890836'
test- map: '0.730207802416', mrr: '0.746080601464

Step 3058: loss = 145.17 (413.948 sec)
valid- map: '0.691770398913', mrr: '0.692592394378'
test- map: '0.712196231484', mrr: '0.724964037058

Step 3495: loss = 111.86 (417.013 sec)
valid- map: '0.696910232625', mrr: '0.70631121524'
test- map: '0.726877209439', mrr: '0.73750192994

Step 3932: loss = 110.50 (413.836 sec)
valid- map: '0.746769280103', mrr: '0.754545168236'
test- map: '0.716325700898', mrr: '0.724240539981

Step 4369: loss = 108.94 (416.093 sec)
valid- map: '0.707945192667', mrr: '0.70824858563'
test- map: '0.711591080958', mrr: '0.727408907964

Step 4806: loss = 146.72 (414.614 sec)
valid- map: '0.711111455719', mrr: '0.715183353839'
test- map: '0.700486436289', mrr: '0.710680454662

Step 5243: loss = 247.34 (415.863 sec)
valid- map: '0.673667238846', mrr: '0.681263610728'
test- map: '0.7035283131', mrr: '0.712358491988

Step 5680: loss = 142.26 (416.656 sec)
valid- map: '0.670912641151', mrr: '0.679727833895'
test- map: '0.717984145464', mrr: '0.732766467643

Step 6117: loss = 115.20 (415.816 sec)
valid- map: '0.71492141016', mrr: '0.72273194178'
test- map: '0.712231870223', mrr: '0.721150956953

Step 6554: loss = 86.07 (416.279 sec)
valid- map: '0.733506837078', mrr: '0.740974083236'
test- map: '0.724700816697', mrr: '0.738764927345

train- map: '0.998401098438', mrr: '1.0'
