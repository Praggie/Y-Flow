Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=5, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3=None, with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1367.03 (341.307 sec)
valid- map: '0.620988087025', mrr: '0.627504211993'
test- map: '0.589756621374', mrr: '0.60586422776

Step 873: loss = 1114.57 (356.788 sec)
valid- map: '0.668668688014', mrr: '0.669386753018'
test- map: '0.674628718345', mrr: '0.691084555591

Step 1310: loss = 802.01 (357.205 sec)
valid- map: '0.694341038885', mrr: '0.698737087428'
test- map: '0.682248159375', mrr: '0.695215444135

Step 1747: loss = 566.63 (354.741 sec)
valid- map: '0.682357016881', mrr: '0.686451247166'
test- map: '0.68354140287', mrr: '0.702750227596

Step 2184: loss = 346.70 (355.668 sec)
valid- map: '0.708431794111', mrr: '0.715861731898'
test- map: '0.687549918495', mrr: '0.705180302094

Step 2621: loss = 187.33 (356.985 sec)
valid- map: '0.710884155527', mrr: '0.711006982436'
test- map: '0.675990356989', mrr: '0.687170834604

Step 3058: loss = 137.17 (357.914 sec)
valid- map: '0.682190839299', mrr: '0.688083381501'
test- map: '0.661735711684', mrr: '0.674845395198

Step 3495: loss = 114.32 (356.928 sec)
valid- map: '0.705982802819', mrr: '0.711570336978'
test- map: '0.673110982206', mrr: '0.688003937078

Step 3932: loss = 85.41 (356.304 sec)
valid- map: '0.69570979065', mrr: '0.700137571864'
test- map: '0.675611724906', mrr: '0.692112912774

Step 4369: loss = 74.32 (358.037 sec)
valid- map: '0.70603140246', mrr: '0.714317494675'
test- map: '0.677860924933', mrr: '0.692375046079

Step 4806: loss = 56.54 (357.344 sec)
valid- map: '0.694350050302', mrr: '0.699728952705'
test- map: '0.660999962543', mrr: '0.678937420604

Step 5243: loss = 58.47 (357.442 sec)
valid- map: '0.707262820953', mrr: '0.714364735793'
test- map: '0.674650119403', mrr: '0.689491224368

Step 5680: loss = 72.79 (356.450 sec)
valid- map: '0.711557453036', mrr: '0.716977297742'
test- map: '0.68464402452', mrr: '0.704370259908

Step 6117: loss = 97.62 (356.027 sec)
valid- map: '0.693489340181', mrr: '0.703805225794'
test- map: '0.705801900813', mrr: '0.722983318991

Step 6554: loss = 86.13 (355.978 sec)
valid- map: '0.717169889194', mrr: '0.720590146185'
test- map: '0.670702306511', mrr: '0.687474423451

train- map: '0.990837595194', mrr: '0.997136311569'
