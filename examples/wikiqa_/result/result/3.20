Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=2, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3=None, with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1147.83 (360.661 sec)
valid- map: '0.671839998626', mrr: '0.683162120067'
test- map: '0.628802294953', mrr: '0.642701011837

Step 873: loss = 813.03 (373.589 sec)
valid- map: '0.72433822791', mrr: '0.727106557464'
test- map: '0.702992648905', mrr: '0.717166418401

Step 1310: loss = 526.33 (372.749 sec)
valid- map: '0.714394170644', mrr: '0.71517679851'
test- map: '0.688212415758', mrr: '0.698758835081

Step 1747: loss = 289.77 (372.240 sec)
valid- map: '0.706181604991', mrr: '0.701300022729'
test- map: '0.706565319075', mrr: '0.717922996609

Step 2184: loss = 176.76 (370.961 sec)
valid- map: '0.717110248658', mrr: '0.728166245131'
test- map: '0.706210642344', mrr: '0.715797216879

Step 2621: loss = 138.39 (373.341 sec)
valid- map: '0.740805193484', mrr: '0.742830747295'
test- map: '0.729791073981', mrr: '0.741094929305

Step 3058: loss = 100.61 (370.899 sec)
valid- map: '0.714630695583', mrr: '0.721018179352'
test- map: '0.698411433524', mrr: '0.706032170847

Step 3495: loss = 98.44 (374.406 sec)
valid- map: '0.740585935229', mrr: '0.746330455259'
test- map: '0.722115247272', mrr: '0.733403686472

Step 3932: loss = 78.94 (373.522 sec)
valid- map: '0.738139935461', mrr: '0.741385400314'
test- map: '0.751292350277', mrr: '0.763456951075

Step 4369: loss = 70.35 (372.077 sec)
valid- map: '0.733805061781', mrr: '0.741344744321'
test- map: '0.727587501355', mrr: '0.74102542002

Step 4806: loss = 87.42 (374.948 sec)
valid- map: '0.741198010543', mrr: '0.745312711979'
test- map: '0.694217026581', mrr: '0.704577343096

Step 5243: loss = 86.45 (371.047 sec)
valid- map: '0.737480420814', mrr: '0.737247364628'
test- map: '0.683997936333', mrr: '0.69496332386

Step 5680: loss = 79.56 (374.047 sec)
valid- map: '0.743807867617', mrr: '0.750138177519'
test- map: '0.711172974914', mrr: '0.720489330057

Step 6117: loss = 65.91 (373.229 sec)
valid- map: '0.743323057014', mrr: '0.748642406976'
test- map: '0.712105540707', mrr: '0.72512623377

Step 6554: loss = 77.76 (373.769 sec)
valid- map: '0.725681934908', mrr: '0.72376394549'
test- map: '0.68775901536', mrr: '0.698590245098

train- map: '0.996415908861', mrr: '0.998854524628'
