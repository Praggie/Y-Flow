Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='hinge_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 181.28 (396.116 sec)
valid- map: '0.693497779024', mrr: '0.694871405398'
test- map: '0.679184778413', mrr: '0.695055344747

Step 873: loss = 66.47 (412.129 sec)
valid- map: '0.681909889053', mrr: '0.680880032666'
test- map: '0.688419498278', mrr: '0.701785744515

Step 1310: loss = 25.80 (410.704 sec)
valid- map: '0.703323755903', mrr: '0.70175688509'
test- map: '0.70760793746', mrr: '0.720007303477

Step 1747: loss = 12.35 (411.336 sec)
valid- map: '0.713995484234', mrr: '0.714751342132'
test- map: '0.696457136644', mrr: '0.705284119173

Step 2184: loss = 5.11 (411.431 sec)
valid- map: '0.721157524134', mrr: '0.72711038961'
test- map: '0.697321580135', mrr: '0.71153352988

Step 2621: loss = 3.82 (411.289 sec)
valid- map: '0.708446800113', mrr: '0.709057498938'
test- map: '0.68546430777', mrr: '0.697754069205

Step 3058: loss = 9.07 (411.103 sec)
valid- map: '0.72193199217', mrr: '0.728419772467'
test- map: '0.687610190026', mrr: '0.692693737447

Step 3495: loss = 8.06 (411.821 sec)
valid- map: '0.71350337273', mrr: '0.716244646007'
test- map: '0.704295824203', mrr: '0.71383841353

Step 3932: loss = 6.97 (410.728 sec)
valid- map: '0.716986904189', mrr: '0.726483943746'
test- map: '0.689973259958', mrr: '0.70124237902

Step 4369: loss = 2.33 (410.796 sec)
valid- map: '0.75998972368', mrr: '0.765601968578'
test- map: '0.705315606226', mrr: '0.714453099484

Step 4806: loss = 4.98 (410.975 sec)
valid- map: '0.692179337417', mrr: '0.700585107133'
test- map: '0.678665422382', mrr: '0.689133026324

Step 5243: loss = 11.27 (411.406 sec)
valid- map: '0.721365385056', mrr: '0.719589603518'
test- map: '0.710102052588', mrr: '0.723502457762

Step 5680: loss = 3.44 (410.839 sec)
valid- map: '0.704458504459', mrr: '0.712254632493'
test- map: '0.679387706135', mrr: '0.689443512592

Step 6117: loss = 1.74 (412.096 sec)
valid- map: '0.714572310406', mrr: '0.712216553288'
test- map: '0.688299560522', mrr: '0.698915048143

Step 6554: loss = 1.27 (410.462 sec)
valid- map: '0.694687521473', mrr: '0.700702604274'
test- map: '0.664355900759', mrr: '0.676235995372

train- map: '0.994818087602', mrr: '0.995704467354'
