Namespace(MP_dim=50, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=2, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='mul', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1148.46 (398.229 sec)
valid- map: '0.636078130423', mrr: '0.638122096158'
test- map: '0.651742014285', mrr: '0.668278158921

Step 873: loss = 752.11 (414.796 sec)
valid- map: '0.707673088229', mrr: '0.712451789833'
test- map: '0.709142397804', mrr: '0.722204201834

Step 1310: loss = 452.00 (412.531 sec)
valid- map: '0.681284445273', mrr: '0.684307876867'
test- map: '0.680777461564', mrr: '0.696019127192

Step 1747: loss = 235.92 (411.361 sec)
valid- map: '0.676433681305', mrr: '0.683034840287'
test- map: '0.68188810562', mrr: '0.697583649955

Step 2184: loss = 127.55 (414.174 sec)
valid- map: '0.69336057094', mrr: '0.701384737397'
test- map: '0.684792082207', mrr: '0.701518760161

Step 2621: loss = 115.32 (414.410 sec)
valid- map: '0.704164607438', mrr: '0.706663662616'
test- map: '0.687210353283', mrr: '0.699463263255

Step 3058: loss = 76.69 (414.562 sec)
valid- map: '0.683574303416', mrr: '0.690407993682'
test- map: '0.691438000241', mrr: '0.703542315561

Step 3495: loss = 72.61 (413.725 sec)
valid- map: '0.701663173687', mrr: '0.705193659956'
test- map: '0.703370935125', mrr: '0.714001464927

Step 3932: loss = 63.46 (413.556 sec)
valid- map: '0.668306674522', mrr: '0.673093774547'
test- map: '0.681415049706', mrr: '0.699286116261

Step 4369: loss = 61.75 (414.648 sec)
valid- map: '0.686016955561', mrr: '0.690899587626'
test- map: '0.704764668345', mrr: '0.718933872483

Step 4806: loss = 47.01 (415.503 sec)
valid- map: '0.680866058545', mrr: '0.689881238691'
test- map: '0.703708315945', mrr: '0.720063509878

Step 5243: loss = 49.55 (414.715 sec)
valid- map: '0.653182597825', mrr: '0.654008227223'
test- map: '0.676605494527', mrr: '0.687628371252

Step 5680: loss = 66.22 (415.335 sec)
valid- map: '0.675496358433', mrr: '0.678681459634'
test- map: '0.668063939594', mrr: '0.679918305826

Step 6117: loss = 109.38 (413.416 sec)
valid- map: '0.660937981771', mrr: '0.664792857055'
test- map: '0.656712729362', mrr: '0.669587286218

Step 6554: loss = 94.67 (415.423 sec)
valid- map: '0.689790996041', mrr: '0.693168736026'
test- map: '0.666249587641', mrr: '0.681166455858

train- map: '0.990786710608', mrr: '0.997709049255'
