Namespace(MP_dim=50, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=3, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='hinge_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3=None, with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 200.94 (348.874 sec)
valid- map: '0.688234098353', mrr: '0.690970933828'
test- map: '0.672725160465', mrr: '0.691214322099

Step 873: loss = 70.29 (363.453 sec)
valid- map: '0.718249045928', mrr: '0.726869459905'
test- map: '0.675493383364', mrr: '0.690779723786

Step 1310: loss = 29.59 (360.556 sec)
valid- map: '0.69744534566', mrr: '0.69629896118'
test- map: '0.702038086992', mrr: '0.717088659238

Step 1747: loss = 10.68 (365.075 sec)
valid- map: '0.671760922059', mrr: '0.671893197191'
test- map: '0.703904107299', mrr: '0.719913655698

Step 2184: loss = 4.30 (361.866 sec)
valid- map: '0.682656012418', mrr: '0.684879978928'
test- map: '0.686031477004', mrr: '0.700816117194

Step 2621: loss = 5.43 (362.038 sec)
valid- map: '0.665099696315', mrr: '0.669162630735'
test- map: '0.697281257296', mrr: '0.713041348227

Step 3058: loss = 15.37 (363.163 sec)
valid- map: '0.670712686784', mrr: '0.67846858829'
test- map: '0.661910565076', mrr: '0.683289287302

Step 3495: loss = 18.12 (363.109 sec)
valid- map: '0.714949795208', mrr: '0.714091541175'
test- map: '0.695647818276', mrr: '0.713775324616

Step 3932: loss = 6.50 (363.680 sec)
valid- map: '0.673494839531', mrr: '0.67477083426'
test- map: '0.690437162806', mrr: '0.704932346427

Step 4369: loss = 3.48 (361.971 sec)
valid- map: '0.685590776067', mrr: '0.688454072383'
test- map: '0.693669604149', mrr: '0.708460673024

Step 4806: loss = 1.11 (364.522 sec)
valid- map: '0.706704406704', mrr: '0.708909476767'
test- map: '0.680842993844', mrr: '0.697002474876

Step 5243: loss = 1.38 (362.671 sec)
valid- map: '0.717329873545', mrr: '0.71658346387'
test- map: '0.702197632522', mrr: '0.717192501143

Step 5680: loss = 1.65 (365.364 sec)
valid- map: '0.699109833003', mrr: '0.693868931929'
test- map: '0.672913519947', mrr: '0.688762825684

Step 6117: loss = 4.28 (365.006 sec)
valid- map: '0.674922641292', mrr: '0.679979500218'
test- map: '0.69015143219', mrr: '0.70968889364

Step 6554: loss = 7.44 (364.331 sec)
valid- map: '0.680988657442', mrr: '0.686686134568'
test- map: '0.678197511222', mrr: '0.697996322379

train- map: '0.984130685493', mrr: '0.986540664376'
