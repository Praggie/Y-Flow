Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=50, attention_type='dot_product', batch_size=40, char_emb_dim=40, char_lstm_dim=80, cnf=1, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=True, is_aggregation_siamese=True, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=13, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='mul', type2=None, type3=None, unstack_cnn=True, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=False, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 893.43 (80.675 sec)
valid- map: '0.686460265925', mrr: '0.689947948877'
test- map: '0.634834426728', mrr: '0.650142585076

Step 545: loss = 772.82 (88.819 sec)
valid- map: '0.679931209296', mrr: '0.683069069379'
test- map: '0.62569583077', mrr: '0.636189125232

Step 818: loss = 709.19 (87.963 sec)
valid- map: '0.682960216591', mrr: '0.686961539342'
test- map: '0.640429900998', mrr: '0.653185750544

Step 1091: loss = 633.74 (88.534 sec)
valid- map: '0.714527734171', mrr: '0.721409190457'
test- map: '0.683461057088', mrr: '0.693786549708

Step 1364: loss = 566.40 (89.323 sec)
valid- map: '0.714071554548', mrr: '0.720505794911'
test- map: '0.683826393086', mrr: '0.69530831537

Step 1637: loss = 491.27 (88.356 sec)
valid- map: '0.712335746264', mrr: '0.723875176851'
test- map: '0.662867169771', mrr: '0.67118989835

Step 1910: loss = 440.51 (87.952 sec)
valid- map: '0.706861726172', mrr: '0.714724223023'
test- map: '0.675320939886', mrr: '0.683735874322

Step 2183: loss = 407.62 (88.563 sec)
valid- map: '0.702019056781', mrr: '0.71107517923'
test- map: '0.682649755546', mrr: '0.691991935819

Step 2456: loss = 318.90 (88.368 sec)
valid- map: '0.734636186124', mrr: '0.744296994892'
test- map: '0.675538002236', mrr: '0.689005130672

Step 2729: loss = 267.24 (88.576 sec)
valid- map: '0.692972096246', mrr: '0.701450302343'
test- map: '0.671546895975', mrr: '0.681389215494

Step 3002: loss = 237.80 (88.408 sec)
valid- map: '0.707687220187', mrr: '0.715982760626'
test- map: '0.667665602753', mrr: '0.67728350753

Step 3275: loss = 190.94 (87.932 sec)
valid- map: '0.691104783664', mrr: '0.701461611581'
test- map: '0.669949160753', mrr: '0.676970069254

Step 3548: loss = 179.78 (87.758 sec)
valid- map: '0.688675477961', mrr: '0.698234415496'
test- map: '0.662740511236', mrr: '0.669958980743

train- map: '0.920665403207', mrr: '0.931139202531'
