Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=100, attention_type='bilinear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=False, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, max_window_size=3, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 988.48 (67.231 sec)
valid- map: '0.569034224728', mrr: '0.57178967034'
test- map: '0.542890887844', mrr: '0.54849184335

Step 545: loss = 867.04 (75.284 sec)
valid- map: '0.618235303057', mrr: '0.627835986765'
test- map: '0.573334965755', mrr: '0.582023914586

Step 818: loss = 840.48 (75.101 sec)
valid- map: '0.605745635473', mrr: '0.610362381161'
test- map: '0.574229196662', mrr: '0.581540554781

Step 1091: loss = 836.38 (75.077 sec)
valid- map: '0.620433358529', mrr: '0.623349710254'
test- map: '0.583572164161', mrr: '0.594643698399

Step 1364: loss = 800.81 (74.201 sec)
valid- map: '0.63526982277', mrr: '0.641830039449'
test- map: '0.580423961579', mrr: '0.589493028691

Step 1637: loss = 828.08 (74.684 sec)
valid- map: '0.629937588866', mrr: '0.636337670266'
test- map: '0.587472790284', mrr: '0.598696852382

Step 1910: loss = 784.33 (75.867 sec)
valid- map: '0.651791011017', mrr: '0.655274256167'
test- map: '0.604180222179', mrr: '0.618078077011

Step 2183: loss = 792.21 (74.188 sec)
valid- map: '0.650353592615', mrr: '0.653975125404'
test- map: '0.60193661463', mrr: '0.613041311864

Step 2456: loss = 773.67 (74.629 sec)
valid- map: '0.662503237503', mrr: '0.666905537144'
test- map: '0.607401186066', mrr: '0.612682342986

Step 2729: loss = 759.37 (74.694 sec)
valid- map: '0.636876791639', mrr: '0.638826275136'
test- map: '0.629366579922', mrr: '0.641427576767

train- map: '0.623526261165', mrr: '0.636033949823'
