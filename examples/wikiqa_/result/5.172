Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=100, attention_type='bilinear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=False, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, max_window_size=3, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 954.96 (66.847 sec)
valid- map: '0.623189376761', mrr: '0.629752742848'
test- map: '0.576340051031', mrr: '0.58333639383

Step 545: loss = 982.74 (74.121 sec)
valid- map: '0.56307537994', mrr: '0.567708922471'
test- map: '0.541029009465', mrr: '0.550116496624

Step 818: loss = 895.38 (73.670 sec)
valid- map: '0.588185282531', mrr: '0.600242790719'
test- map: '0.540012806103', mrr: '0.541351559119

Step 1091: loss = 884.21 (74.270 sec)
valid- map: '0.591276999015', mrr: '0.594486245677'
test- map: '0.549441558873', mrr: '0.554065637913

Step 1364: loss = 893.79 (74.317 sec)
valid- map: '0.581562364003', mrr: '0.591591653496'
test- map: '0.53540506301', mrr: '0.538054425536

Step 1637: loss = 858.47 (74.599 sec)
valid- map: '0.604424807996', mrr: '0.612096765668'
test- map: '0.565290507368', mrr: '0.57485677692

Step 1910: loss = 850.92 (74.251 sec)
valid- map: '0.601697905269', mrr: '0.608840762412'
test- map: '0.56068659161', mrr: '0.566743997281

Step 2183: loss = 802.69 (75.324 sec)
valid- map: '0.599570718817', mrr: '0.606339559911'
test- map: '0.548061713316', mrr: '0.557640382023

Step 2456: loss = 788.19 (73.563 sec)
valid- map: '0.616137338126', mrr: '0.623239252966'
test- map: '0.581652620289', mrr: '0.589766105489

Step 2729: loss = 716.43 (74.316 sec)
valid- map: '0.611223807057', mrr: '0.615273945631'
test- map: '0.574186857147', mrr: '0.583486034394

train- map: '0.615346091128', mrr: '0.627822192126'
