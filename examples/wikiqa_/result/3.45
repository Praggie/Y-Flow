Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3=None, with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 1893.15 (350.741 sec)
valid- map: '0.68022694492', mrr: '0.685275445802'
test- map: '0.636757644427', mrr: '0.649681278941

Step 873: loss = 1068.16 (365.540 sec)
valid- map: '0.681755053744', mrr: '0.689682113457'
test- map: '0.676498823957', mrr: '0.695002688213

Step 1310: loss = 659.65 (365.580 sec)
valid- map: '0.693405939169', mrr: '0.703456184339'
test- map: '0.674308596819', mrr: '0.688787918937

Step 1747: loss = 417.16 (366.621 sec)
valid- map: '0.672027333337', mrr: '0.685011769536'
test- map: '0.6693621993', mrr: '0.685459682065

Step 2184: loss = 274.11 (368.599 sec)
valid- map: '0.675498149273', mrr: '0.683576094255'
test- map: '0.695434870069', mrr: '0.706799402015

Step 2621: loss = 239.81 (366.833 sec)
valid- map: '0.707174123211', mrr: '0.719715550038'
test- map: '0.680210671782', mrr: '0.691013988835

Step 3058: loss = 181.31 (368.457 sec)
valid- map: '0.717065282949', mrr: '0.724073200076'
test- map: '0.691809393012', mrr: '0.701271773494

Step 3495: loss = 154.20 (366.625 sec)
valid- map: '0.701854516735', mrr: '0.709468694885'
test- map: '0.672941099815', mrr: '0.682837026337

Step 3932: loss = 142.56 (365.610 sec)
valid- map: '0.7042401231', mrr: '0.712958168723'
test- map: '0.707185671067', mrr: '0.718804830824

Step 4369: loss = 123.01 (364.735 sec)
valid- map: '0.708399845305', mrr: '0.71296668499'
test- map: '0.697288021457', mrr: '0.709750681955

Step 4806: loss = 128.08 (364.762 sec)
valid- map: '0.738070042832', mrr: '0.74408856135'
test- map: '0.700160432275', mrr: '0.714110774913

Step 5243: loss = 147.02 (365.077 sec)
valid- map: '0.699336653206', mrr: '0.707520103353'
test- map: '0.687320869208', mrr: '0.7008208106

Step 5680: loss = 174.21 (365.384 sec)
valid- map: '0.721217076574', mrr: '0.73087316123'
test- map: '0.66593460535', mrr: '0.680917733387

Step 6117: loss = 149.72 (368.745 sec)
valid- map: '0.695018179507', mrr: '0.702702734811'
test- map: '0.678815212695', mrr: '0.690565904146

Step 6554: loss = 148.18 (370.727 sec)
valid- map: '0.714487276392', mrr: '0.726215671454'
test- map: '0.6873840169', mrr: '0.696569576971

train- map: '0.993069076145', mrr: '0.996849942726'
