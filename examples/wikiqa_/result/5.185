Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=100, attention_type='bilinear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=False, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, max_window_size=3, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 962.59 (67.235 sec)
valid- map: '0.607978063236', mrr: '0.615069718641'
test- map: '0.575115677088', mrr: '0.583172303234

Step 545: loss = 861.90 (74.106 sec)
valid- map: '0.611914298224', mrr: '0.618751376489'
test- map: '0.579696190457', mrr: '0.585052675641

Step 818: loss = 841.99 (72.923 sec)
valid- map: '0.630288142193', mrr: '0.63692709407'
test- map: '0.611552256807', mrr: '0.62129124876

Step 1091: loss = 882.14 (73.411 sec)
valid- map: '0.597498378614', mrr: '0.603061282788'
test- map: '0.589767219599', mrr: '0.596577944311

Step 1364: loss = 836.31 (73.550 sec)
valid- map: '0.625238066607', mrr: '0.63161246822'
test- map: '0.60637456013', mrr: '0.609103644032

Step 1637: loss = 837.41 (74.244 sec)
valid- map: '0.602420892599', mrr: '0.611454969193'
test- map: '0.582980603517', mrr: '0.588969061605

Step 1910: loss = 802.76 (73.390 sec)
valid- map: '0.626218820862', mrr: '0.634674981104'
test- map: '0.60078763026', mrr: '0.607756845349

Step 2183: loss = 782.02 (73.478 sec)
valid- map: '0.646193591442', mrr: '0.654455538196'
test- map: '0.601281742008', mrr: '0.608197520437

Step 2456: loss = 750.20 (73.710 sec)
valid- map: '0.62557313718', mrr: '0.634931602789'
test- map: '0.585202732191', mrr: '0.590942105986

Step 2729: loss = 755.42 (73.866 sec)
valid- map: '0.631449113404', mrr: '0.635659871781'
test- map: '0.590375124922', mrr: '0.596610370049

train- map: '0.627514156879', mrr: '0.637741119501'
