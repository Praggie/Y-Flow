Namespace(MP_dim=50, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.1, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=5, model_dir='../models', modify_loss=0, optimize_type='adam', prediction_mode='hinge_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='mul', with_NER=False, with_POS=False, with_aggregation_highway=True, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 204.82 (391.618 sec)
valid- map: '0.677718027123', mrr: '0.678852298495'
test- map: '0.674651102671', mrr: '0.691293173469

Step 873: loss = 75.16 (409.890 sec)
valid- map: '0.743520093222', mrr: '0.748976442429'
test- map: '0.665127316671', mrr: '0.675986793271

Step 1310: loss = 38.82 (410.972 sec)
valid- map: '0.735077620792', mrr: '0.736366213152'
test- map: '0.701953919721', mrr: '0.711823007811

Step 1747: loss = 14.93 (410.434 sec)
valid- map: '0.731574675325', mrr: '0.731484917199'
test- map: '0.715612659042', mrr: '0.72068117807

Step 2184: loss = 6.18 (410.681 sec)
valid- map: '0.689886568458', mrr: '0.689036712846'
test- map: '0.693362136263', mrr: '0.702266138069

Step 2621: loss = 5.46 (409.939 sec)
valid- map: '0.705804160566', mrr: '0.706321147988'
test- map: '0.719121594956', mrr: '0.730729189044

Step 3058: loss = 7.42 (409.940 sec)
valid- map: '0.714620641704', mrr: '0.71994047619'
test- map: '0.715497011562', mrr: '0.727374420276

Step 3495: loss = 5.84 (412.555 sec)
valid- map: '0.727752798696', mrr: '0.734933933258'
test- map: '0.709256522122', mrr: '0.723082324626

Step 3932: loss = 4.73 (413.267 sec)
valid- map: '0.702881223715', mrr: '0.711684303351'
test- map: '0.696650008052', mrr: '0.710772035463

Step 4369: loss = 5.94 (408.573 sec)
valid- map: '0.715481776756', mrr: '0.722858174609'
test- map: '0.699251080117', mrr: '0.717042411178

Step 4806: loss = 2.91 (412.972 sec)
valid- map: '0.73000716215', mrr: '0.727570004951'
test- map: '0.724754203219', mrr: '0.737983838292

Step 5243: loss = 4.61 (410.956 sec)
valid- map: '0.699607740878', mrr: '0.704793200627'
test- map: '0.716894797875', mrr: '0.727609770202

Step 5680: loss = 4.02 (408.000 sec)
valid- map: '0.710304634674', mrr: '0.71588272061'
test- map: '0.711802333856', mrr: '0.723630572069

Step 6117: loss = 7.13 (407.976 sec)
valid- map: '0.690713970773', mrr: '0.690770660116'
test- map: '0.702374292354', mrr: '0.712737251317

Step 6554: loss = 7.15 (412.182 sec)
valid- map: '0.710282904926', mrr: '0.709306302163'
test- map: '0.724451200738', mrr: '0.73910200814

train- map: '0.989287987054', mrr: '0.990999836361'
