Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=100, attention_type='bilinear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=False, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=10, max_sent_length=100, max_window_size=3, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='w_sub_mul', unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=False, with_filter_layer=False, with_highway=True, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 967.26 (64.840 sec)
valid- map: '0.618920156123', mrr: '0.619123292933'
test- map: '0.56454395806', mrr: '0.571679909026

Step 545: loss = 910.06 (71.493 sec)
valid- map: '0.606249515476', mrr: '0.608159631374'
test- map: '0.586370423507', mrr: '0.594379140214

Step 818: loss = 894.33 (71.296 sec)
valid- map: '0.601782652973', mrr: '0.606323812871'
test- map: '0.577881867948', mrr: '0.588380484579

Step 1091: loss = 935.41 (72.952 sec)
valid- map: '0.526440681168', mrr: '0.52449119767'
test- map: '0.541413636841', mrr: '0.545259482353

Step 1364: loss = 1027.53 (71.776 sec)
valid- map: '0.516220970388', mrr: '0.521083656203'
test- map: '0.497355716918', mrr: '0.507737339183

Step 1637: loss = 1001.67 (71.874 sec)
valid- map: '0.476929158739', mrr: '0.485735101474'
test- map: '0.451555839471', mrr: '0.454881136431

Step 1910: loss = 1019.89 (71.252 sec)
valid- map: '0.399852997458', mrr: '0.408252794829'
test- map: '0.37425240112', mrr: '0.385079870975

Step 2183: loss = 1023.39 (71.825 sec)
valid- map: '0.464806602824', mrr: '0.482870516423'
test- map: '0.431819681032', mrr: '0.435450426883

Step 2456: loss = 1023.20 (71.754 sec)
valid- map: '0.455444100126', mrr: '0.465817963476'
test- map: '0.41195310034', mrr: '0.417022860541

Step 2729: loss = 1032.89 (71.346 sec)
valid- map: '0.363470205566', mrr: '0.365874983176'
test- map: '0.338225092913', mrr: '0.337842316144

train- map: '0.287855862972', mrr: '0.289617919339'
