Namespace(MP_dim=20, NER_dim=20, POS_dim=20, aggregation_layer_num=2, aggregation_lstm_dim=80, batch_size=60, char_emb_dim=40, char_lstm_dim=80, context_layer_num=2, context_lstm_dim=100, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=0, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=False, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=15, max_sent_length=100, max_window_size=5, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='w_sub_mul', type2='mul', type3='mul', with_NER=False, with_POS=False, with_aggregation_highway=False, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=True, wo_agg_self_att=True, wo_attentive_match=True, wo_bilinear_att=False, wo_char=False, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.840B.300d.txt')

Step 436: loss = 2538.33 (421.001 sec)
valid- map: '0.614836115099', mrr: '0.628631810144'
test- map: '0.612029771963', mrr: '0.627670934461

Step 873: loss = 1622.54 (435.652 sec)
valid- map: '0.646784512821', mrr: '0.655804901603'
test- map: '0.654462177651', mrr: '0.66568597726

Step 1310: loss = 1126.50 (437.238 sec)
valid- map: '0.671184048168', mrr: '0.670580896176'
test- map: '0.678828021556', mrr: '0.6943782041

Step 1747: loss = 694.87 (437.509 sec)
valid- map: '0.680279691519', mrr: '0.682232324424'
test- map: '0.670482446711', mrr: '0.686278937874

Step 2184: loss = 430.10 (436.853 sec)
valid- map: '0.681369192976', mrr: '0.688557716534'
test- map: '0.660503183671', mrr: '0.672438346975

Step 2621: loss = 244.76 (434.394 sec)
valid- map: '0.695873841637', mrr: '0.701879021487'
test- map: '0.67850516226', mrr: '0.69179054142

Step 3058: loss = 171.53 (436.472 sec)
valid- map: '0.714082000321', mrr: '0.717555797248'
test- map: '0.67228611594', mrr: '0.688334773366

Step 3495: loss = 146.57 (435.396 sec)
valid- map: '0.658753202136', mrr: '0.664828409996'
test- map: '0.668138610361', mrr: '0.681750114312

Step 3932: loss = 142.80 (439.831 sec)
valid- map: '0.706361892076', mrr: '0.71033615855'
test- map: '0.666656855947', mrr: '0.680013704859

Step 4369: loss = 116.07 (437.949 sec)
valid- map: '0.688343556796', mrr: '0.6930424734'
test- map: '0.673717446949', mrr: '0.690644038175

Step 4806: loss = 120.33 (441.448 sec)
valid- map: '0.689387387304', mrr: '0.696024764477'
test- map: '0.673605625627', mrr: '0.688408111093

Step 5243: loss = 140.78 (441.776 sec)
valid- map: '0.704111353814', mrr: '0.704405823453'
test- map: '0.653320608572', mrr: '0.66528199335

Step 5680: loss = 151.93 (444.285 sec)
valid- map: '0.729122376741', mrr: '0.735745581579'
test- map: '0.663174325308', mrr: '0.676327511261

Step 6117: loss = 145.89 (441.890 sec)
valid- map: '0.687803664589', mrr: '0.690279099208'
test- map: '0.685861424959', mrr: '0.699072195099

Step 6554: loss = 112.19 (440.486 sec)
valid- map: '0.709362506982', mrr: '0.714731961161'
test- map: '0.684564166636', mrr: '0.70034379772

train- map: '0.993541762743', mrr: '0.997136311569'
