Namespace(MP_dim=100, NER_dim=20, POS_dim=20, aggregation_layer_num=1, aggregation_lstm_dim=50, attention_type='bilinear', batch_size=40, char_emb_dim=40, char_lstm_dim=80, cnf=5, context_layer_num=1, context_lstm_dim=50, dev_path='../data/wikiqa/WikiQACorpus/WikiQA-dev.txt', dropout_rate=0.04, fix_word_vec=True, highway_layer_num=1, is_aggregation_lstm=True, is_aggregation_siamese=False, is_answer_selection=True, is_server=True, is_shared_attention=True, lambda_l2=0.0, learning_rate=0.002, lex_decompsition_dim=-1, max_char_per_word=10, max_epochs=13, max_sent_length=100, max_window_size=1, model_dir='../models', modify_loss=0.1, optimize_type='adam', prediction_mode='list_wise', suffix='normal', test_path='../data/wikiqa/WikiQACorpus/WikiQA-test.txt', train_path='../data/wikiqa/WikiQACorpus/WikiQA-train.txt', type1='mul', type2='mul', type3=None, unstack_cnn=False, with_NER=False, with_POS=False, with_aggregation_highway=False, with_context_self_attention=False, with_filter_layer=False, with_highway=False, with_lex_decomposition=False, with_match_highway=False, wo_agg_self_att=True, wo_attentive_match=True, wo_char=True, wo_full_match=True, wo_left_match=False, wo_lstm_drop_out=True, wo_max_attentive_match=True, wo_maxpool_match=True, wo_right_match=False, word_level_MP_dim=-1, word_vec_path='../data/glove/glove.6B.50d.txt')

Step 272: loss = 940.60 (89.692 sec)
valid- map: '0.647070789595', mrr: '0.64981549859'
test- map: '0.591693745728', mrr: '0.602129185401

Step 545: loss = 820.97 (98.407 sec)
valid- map: '0.676364497495', mrr: '0.682028421314'
test- map: '0.616034258741', mrr: '0.625830542343

Step 818: loss = 731.01 (98.304 sec)
valid- map: '0.651268297399', mrr: '0.655801583778'
test- map: '0.597638747414', mrr: '0.609398218678

Step 1091: loss = 649.05 (98.099 sec)
valid- map: '0.662559680526', mrr: '0.667795284869'
test- map: '0.64153535037', mrr: '0.648761610084

Step 1364: loss = 583.36 (98.803 sec)
valid- map: '0.689823404109', mrr: '0.696739503882'
test- map: '0.653254991431', mrr: '0.659861594738

Step 1637: loss = 508.96 (99.438 sec)
valid- map: '0.707075074744', mrr: '0.711835889326'
test- map: '0.66477787288', mrr: '0.671206175121

Step 1910: loss = 493.90 (96.690 sec)
valid- map: '0.683501349474', mrr: '0.692238856822'
test- map: '0.669822824818', mrr: '0.672777043667

Step 2183: loss = 356.45 (97.962 sec)
valid- map: '0.676940266524', mrr: '0.682416855631'
test- map: '0.664022363348', mrr: '0.671626597534

Step 2456: loss = 300.07 (96.861 sec)
valid- map: '0.683185042819', mrr: '0.688353705725'
test- map: '0.663768690909', mrr: '0.671163316745

Step 2729: loss = 244.27 (96.883 sec)
valid- map: '0.696833876001', mrr: '0.702757912282'
test- map: '0.676165054619', mrr: '0.687666779951

Step 3002: loss = 214.82 (96.963 sec)
valid- map: '0.674740834562', mrr: '0.675171213266'
test- map: '0.668691691009', mrr: '0.682016067104

Step 3275: loss = 162.25 (98.125 sec)
valid- map: '0.686831037129', mrr: '0.695071176619'
test- map: '0.668053197374', mrr: '0.676177160745

Step 3548: loss = 134.48 (98.252 sec)
valid- map: '0.714194795852', mrr: '0.71945273236'
test- map: '0.649125047448', mrr: '0.661694782664

train- map: '0.963867524274', mrr: '0.972519500355'
